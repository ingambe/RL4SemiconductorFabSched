import argparse
import os
import random
import sys
import math
import time
import cloudpickle
from tqdm import tqdm
from typing import Dict, List

import numpy as np
import torch
import wandb
import ray
from torch import nn
from torch.utils.data import TensorDataset, DataLoader

from simulation.classes import Lot
from simulation.dispatching.dispatcher import Dispatchers
from simulation.file_instance import FileInstance
from simulation.greedy import build_batch
from simulation.instance import Instance
from simulation.plugins.cost_plugin import PierreCostPlugin
from simulation.randomizer import Randomizer
from simulation.read import read_all


def _center_function(population_size):
    centers = np.arange(0, population_size, dtype=np.float32)
    centers = centers / (population_size - 1)
    centers -= 0.5
    return centers


def _compute_ranks(rewards):
    rewards = np.array(rewards)
    ranks = np.empty(rewards.size, dtype=int)
    ranks[rewards.argsort()] = np.arange(rewards.size)
    return ranks


def rank_transformation(rewards):
    ranks = _compute_ranks(rewards)
    values = _center_function(rewards.size)
    return values[ranks]


def _no_grad_trunc_normal_(tensor, mean, std, a, b):
    # Cut & paste from PyTorch official master until it's in a few official releases - RW
    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1. + math.erf(x / math.sqrt(2.))) / 2.

    with torch.no_grad():
        # Values are generated by using a truncated uniform distribution and
        # then using the inverse CDF for the normal distribution.
        # Get upper and lower cdf values
        l = norm_cdf((a - mean) / std)
        u = norm_cdf((b - mean) / std)

        # Uniformly fill tensor with values from [l, u], then translate to
        # [2l-1, 2u-1].
        tensor.uniform_(2 * l - 1, 2 * u - 1)

        # Use inverse cdf transform for normal distribution to get truncated
        # standard normal
        tensor.erfinv_()

        # Transform to proper mean, std
        tensor.mul_(std * math.sqrt(2.))
        tensor.add_(mean)

        # Clamp to ensure it's in the proper range
        tensor.clamp_(min=a, max=b)
        return tensor


def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):
    # type: (Tensor, float, float, float, float) -> Tensor
    return _no_grad_trunc_normal_(tensor, mean, std, a, b)


def init_weights(m: nn.Linear, std=math.sqrt(2)):
    torch.nn.init.orthogonal_(m.weight, gain=std)
    if m.bias is not None:
        torch.nn.init.constant_(m.bias, 0)
    return m


class PositionalEncoding(nn.Module):

    def __init__(self, embedding_len, imput_dim):
        super(PositionalEncoding, self).__init__()
        self.embedding = nn.Parameter(torch.zeros(embedding_len, imput_dim))
        trunc_normal_(self.embedding, std=.02)

    def forward(self, x, i):
        x = x + self.embedding[i]
        return x


class SelfSupervision(nn.Module):

    def __init__(self, imput_dim, embedding_len):
        super(SelfSupervision, self).__init__()
        self.embedding = PositionalEncoding(embedding_len, imput_dim)
        self.attention_head = nn.MultiheadAttention(imput_dim, 2, bias=False, add_bias_kv=False, batch_first=True)
        self.output = nn.Linear(imput_dim, embedding_len, bias=False)
        torch.nn.init.xavier_normal_(self.output.weight)

    def forward(self, x, i):
        x = self.embedding(x, i)
        x = x.unsqueeze(0)
        x, _ = self.attention_head.forward(x, x, x, need_weights=False)
        x = x.squeeze(0)
        return self.output(x)


class SecondNet(nn.Module):

    def __init__(self, imput_dim):
        super(SecondNet, self).__init__()
        self.first_attention_head = nn.MultiheadAttention(imput_dim, 2, bias=False, add_bias_kv=False, batch_first=True)
        self.alpha = nn.Parameter(torch.full((1,), 1.0, dtype=torch.float))
        self.beta = nn.Parameter(torch.full((1,), 1.0, dtype=torch.float))
        # self.first_attention_head.in_proj_weight.data = torch.randn_like(self.first_attention_head.in_proj_weight.data)
        # self.first_attention_head.in_proj_weight.data *= 1.0 / torch.sqrt(
        #    torch.square(self.first_attention_head.in_proj_weight.data).sum(dim=0, keepdim=True))
        # self.first_attention_head.out_proj = init_weights(self.first_attention_head.out_proj)
        self.linear = nn.Sequential(
            init_weights(nn.Linear(imput_dim, 16)),
            nn.Tanh(),
            init_weights(nn.Linear(16, 16)),
            nn.Tanh(),
            init_weights(nn.Linear(16, 1, bias=False), std=0.01)
        )

    def forward(self, x):
        x_init = (self.alpha * x)
        x = x_init.unsqueeze(0)
        x, _ = self.first_attention_head.forward(x, x, x, need_weights=False)
        x = x.squeeze(0)
        # x = torch.cat((x, x_init), dim=1)
        x = x_init + (self.beta * x)
        return self.linear(x)


class Normalizer(nn.Module):

    def __init__(self, imput_dim):
        super(Normalizer, self).__init__()
        self.mean = nn.Parameter(torch.zeros((imput_dim,), dtype=torch.float))
        self.std = nn.Parameter(torch.zeros((imput_dim,), dtype=torch.float))

    def forward(self, x):
        return (x - self.mean) / (self.std + 1e-3)


class Network(nn.Module):

    def __init__(self, imput_dim, embedding_len):
        super(Network, self).__init__()
        self.normalizer = Normalizer(imput_dim)
        self.embedding = PositionalEncoding(embedding_len, imput_dim)
        self.inner_net = SecondNet(imput_dim)

    def forward(self, x_init, i):
        x_init = self.normalizer(x_init)
        x = self.embedding(x_init, i)
        return self.inner_net(x)


def get_lot_priority(lots: List, instance: Instance, current_time: int, tools_type: list, neural_net: nn.Module):
    with torch.no_grad():
        percent_processed = []
        left_to_deadline = []
        total_waiting = []
        waiting_last_step = []
        lot_to_lens = []
        priorities = []
        remaining_times = []
        min_setup_times = []
        current_op_mean = []
        nb_machine_set_up = []
        min_batching = []
        max_batching = []
        tools_type_required = []

        # used to store the type of tool needed
        free_tools_type = []
        for lot in lots:
            percent_processed.append((lot.full_time - lot.remaining_time) / lot.full_time)
            left_to_deadline.append(lot.deadline_at - current_time)
            total_waiting.append(lot.waiting_time)
            waiting_last_step.append(current_time - lot.free_since)
            lot_to_lens.append(len(lot.dedications))
            priorities.append(lot.priority)
            remaining_times.append(lot.remaining_time)
            current_op_mean.append(lot.actual_step.processing_time.avg())
            min_setup_time = float('inf')
            tmp_nb = 0
            if lot.actual_step.setup_needed != '':
                for machine in lot.waiting_machines:
                    if machine.current_setup != lot.actual_step.setup_needed:
                        if lot.actual_step.setup_time is not None:
                            min_setup_time = min(min_setup_time, lot.actual_step.setup_time)
                        elif (machine.current_setup, lot.actual_step.setup_needed) in instance.setups:
                            min_setup_time = min(min_setup_time,
                                                 instance.setups[(machine.current_setup, lot.actual_step.setup_needed)])
                        elif ('', lot.actual_step.setup_needed) in instance.setups:
                            min_setup_time = min(min_setup_time, instance.setups[('', lot.actual_step.setup_needed)])
                        else:
                            min_setup_time = 0
                    else:
                        tmp_nb += 1
                        min_setup_time = 0
            else:
                min_setup_time = 0
                tmp_nb = len(lot.waiting_machines)

            min_setup_times.append(min_setup_time)
            min_batching.append(lot.actual_step.batch_min)
            max_batching.append(lot.actual_step.batch_max)
            # matrix dedication
            free_tools_type.append(lot.waiting_machines[0].group)
            nb_machine_set_up.append(tmp_nb)
            tools_type_required.append(lot.actual_step.family)

        percent_processed = torch.FloatTensor(percent_processed).reshape(-1, 1)
        left_to_deadline = torch.FloatTensor(left_to_deadline).reshape(-1, 1)
        total_waiting = torch.FloatTensor(total_waiting).reshape(-1, 1)
        waiting_last_step = torch.FloatTensor(waiting_last_step).reshape(-1, 1)
        lot_to_lens = torch.FloatTensor(lot_to_lens).reshape(-1, 1)
        priorities = torch.FloatTensor(priorities).reshape(-1, 1)
        remaining_times = torch.FloatTensor(remaining_times).reshape(-1, 1)
        min_setup_times = torch.FloatTensor(min_setup_times).reshape(-1, 1)
        current_op_mean = torch.FloatTensor(current_op_mean).reshape(-1, 1)
        nb_machine_set_up = torch.FloatTensor(nb_machine_set_up).reshape(-1, 1)
        min_batching = torch.FloatTensor(min_batching).reshape(-1, 1)
        max_batching = torch.FloatTensor(max_batching).reshape(-1, 1)
        lots_rep = torch.hstack((percent_processed,
                                 left_to_deadline,
                                 total_waiting,
                                 waiting_last_step,
                                 lot_to_lens,
                                 priorities,
                                 remaining_times,
                                 min_setup_times,
                                 current_op_mean,
                                 nb_machine_set_up,
                                 min_batching,
                                 max_batching))
        free_tools_type = [tools_type.index(x) if not x.startswith('Delay_') else tools_type.index('Delay') for x in
                           free_tools_type]
        '''
        matrix_machine = torch.eye(len(lots), dtype=torch.bool)
        tools_type_required = np.array(tools_type_required)
        uniques, count_tools = np.unique(tools_type_required, return_counts=True)
        if len(uniques) < len(tools_type_required):
            for unique_tool, count_tool in zip(uniques, count_tools):
                if count_tool > 1:
                    idxs = np.where(tools_type_required == unique_tool)[0]
                    for i in idxs:
                        matrix_machine[i, idxs] = True
        '''

        priority_jobs = neural_net(lots_rep, torch.LongTensor(free_tools_type)).view(-1)
        # order_jobs = torch.argsort(priority_jobs, descending=True)
        return priority_jobs


def get_lots_to_dispatch_by_lot(instance, current_time, tools_type: list, net: nn.Module, last_sort_time: float):
    if last_sort_time != current_time:
        if len(instance.usable_lots) > 1:
            priority_jobs = get_lot_priority(instance.usable_lots, instance, current_time, tools_type, net)
            for lot, associated_priority in zip(instance.usable_lots, priority_jobs):
                lot.ptuple = (
                    0 if lot.cqt_waiting is not None else 1,
                    -lot.priority,
                    associated_priority
                )
            instance.usable_lots.sort(key=lambda k: k.ptuple)
        last_sort_time = current_time
    lots = instance.usable_lots
    min_set_time_for_machine = float('inf')
    setup_machine, setup_batch = None, None
    family_lock = None
    for i in range(len(lots)):
        lot: Lot = lots[i]
        if family_lock is None or family_lock == lot.actual_step.family:
            family_lock = lot.actual_step.family
            if lot.actual_step.setup_needed == '':
                most_waiting_machine_no_setup = (-1, float('inf'))
                most_waiting_machine = (-1, float('inf'))
                for machine_index, machine in enumerate(lot.waiting_machines):
                    if machine.current_setup == '':
                        if most_waiting_machine_no_setup[1] > machine.free_since:
                            most_waiting_machine_no_setup = (machine_index, machine.free_since)
                        if most_waiting_machine[1] > machine.free_since:
                            most_waiting_machine = (machine_index, machine.free_since)
                if most_waiting_machine_no_setup[0] != -1:
                    return lot.waiting_machines[most_waiting_machine_no_setup[0]], build_batch(lot, lots[i + 1:]), last_sort_time
                else:
                    return lot.waiting_machines[most_waiting_machine[0]], build_batch(lot, lots[i + 1:]), last_sort_time
            else:
                for machine in lot.waiting_machines:
                    if lot.actual_step.setup_needed == machine.current_setup:
                        return machine, build_batch(lot, lots[i + 1:]), last_sort_time
                    else:
                        setup_time_for_machine = Dispatchers.get_setup(lot.actual_step.setup_needed, machine, lot.actual_step.setup_time, instance.setups)
                        if setup_machine is None:
                            setup_machine = machine
                            setup_batch = i
                        if machine.min_runs_left is None:
                            if min_set_time_for_machine > setup_time_for_machine:
                                min_set_time_for_machine = setup_time_for_machine
                                setup_machine = machine
                                setup_batch = i
    return setup_machine, build_batch(lots[setup_batch], lots[setup_batch + 1:]), last_sort_time


@ray.remote
def one_pop_iter(random_seed: int, run_to: int, tools_type: list, net: nn.Module, dataset: str):
    with torch.no_grad():
        Randomizer().random.seed(random_seed)
        random.seed(random_seed)
        os.environ['PYTHONHASHSEED'] = str(random_seed)
        np.random.seed(random_seed)
        torch.manual_seed(random_seed)
        l4m = False
        plugins = [PierreCostPlugin()]
        # instance = FileInstance(files, run_to, l4m, plugins)
        if dataset == 'HVLM':
            with open("365days_HVLM.pkl", mode="rb") as file:
                instance = cloudpickle.load(file, fix_imports=True)
                instance.plugins = plugins
                for plugin in plugins:
                    plugin.on_sim_init(instance)
        elif dataset == 'LVHM':
            with open("365days_LVHM.pkl", mode="rb") as file:
                instance = cloudpickle.load(file, fix_imports=True)
                instance.plugins = plugins
                for plugin in plugins:
                    plugin.on_sim_init(instance)
        net = torch.jit.script(net)
        last_sort_time = -1
        while not instance.done:
            done = instance.next_decision_point()
            if done or instance.current_time > run_to:
                break
            machine, lots, last_sort_time = get_lots_to_dispatch_by_lot(instance, instance.current_time, tools_type,
                                                                        net, last_sort_time)
            if lots is None:
                instance.usable_lots.clear()
                instance.lot_in_usable.clear()
                instance.next_step()
            else:
                instance.dispatch(machine, lots)
        instance.finalize()
        return instance.plugins[0].get_output_value()


def get_lot_representation(instance, current_time, tools_type: list, last_sort_time: float, lot_rep_self, targets_lot):
    if last_sort_time != current_time:
        percent_processed = []
        left_to_deadline = []
        total_waiting = []
        waiting_last_step = []
        lot_to_lens = []
        priorities = []
        remaining_times = []
        current_op_mean = []
        nb_machine_set_up = []
        min_batching = []
        max_batching = []
        min_setup_times = []
        tools_type_required = []

        # used to store the type of tool needed
        free_tools_type = []
        for lot in instance.usable_lots:
            lot.ptuple = Dispatchers.cr_ptuple_for_lot(lot, current_time, None)

            percent_processed.append((lot.full_time - lot.remaining_time) / lot.full_time)
            left_to_deadline.append(lot.deadline_at - current_time)
            total_waiting.append(lot.waiting_time)
            waiting_last_step.append(current_time - lot.free_since)
            lot_to_lens.append(len(lot.dedications))
            priorities.append(lot.priority)
            remaining_times.append(lot.remaining_time)
            current_op_mean.append(lot.actual_step.processing_time.avg())
            min_setup_time = float('inf')
            tmp_nb = 0
            if lot.actual_step.setup_needed != '':
                for machine in lot.waiting_machines:
                    if machine.current_setup != lot.actual_step.setup_needed:
                        if lot.actual_step.setup_time is not None:
                            min_setup_time = min(min_setup_time, lot.actual_step.setup_time)
                        elif (machine.current_setup, lot.actual_step.setup_needed) in instance.setups:
                            min_setup_time = min(min_setup_time,
                                                 instance.setups[(machine.current_setup, lot.actual_step.setup_needed)])
                        elif ('', lot.actual_step.setup_needed) in instance.setups:
                            min_setup_time = min(min_setup_time, instance.setups[('', lot.actual_step.setup_needed)])
                        else:
                            min_setup_time = 0
                    else:
                        tmp_nb += 1
                        min_setup_time = 0
            else:
                min_setup_time = 0
                tmp_nb = len(lot.waiting_machines)

            min_setup_times.append(min_setup_time)
            min_batching.append(lot.actual_step.batch_min)
            max_batching.append(lot.actual_step.batch_max)
            # matrix dedication
            free_tools_type.append(lot.waiting_machines[0].group)
            nb_machine_set_up.append(tmp_nb)
            tools_type_required.append(lot.actual_step.family)

        percent_processed = torch.FloatTensor(percent_processed).reshape(-1, 1)
        left_to_deadline = torch.FloatTensor(left_to_deadline).reshape(-1, 1)
        total_waiting = torch.FloatTensor(total_waiting).reshape(-1, 1)
        waiting_last_step = torch.FloatTensor(waiting_last_step).reshape(-1, 1)
        lot_to_lens = torch.FloatTensor(lot_to_lens).reshape(-1, 1)
        priorities = torch.FloatTensor(priorities).reshape(-1, 1)
        remaining_times = torch.FloatTensor(remaining_times).reshape(-1, 1)
        min_setup_times = torch.FloatTensor(min_setup_times).reshape(-1, 1)
        current_op_mean = torch.FloatTensor(current_op_mean).reshape(-1, 1)
        nb_machine_set_up = torch.FloatTensor(nb_machine_set_up).reshape(-1, 1)
        min_batching = torch.FloatTensor(min_batching).reshape(-1, 1)
        max_batching = torch.FloatTensor(max_batching).reshape(-1, 1)
        lots_rep = torch.hstack((percent_processed,
                                 left_to_deadline,
                                 total_waiting,
                                 waiting_last_step,
                                 lot_to_lens,
                                 priorities,
                                 remaining_times,
                                 min_setup_times,
                                 current_op_mean,
                                 nb_machine_set_up,
                                 min_batching,
                                 max_batching))
        lot_rep_self.append(lots_rep)
        free_tools_type = [tools_type.index(x) if not x.startswith('Delay_') else tools_type.index('Delay') for x in
                           free_tools_type]
        target = torch.LongTensor(free_tools_type)
        targets_lot.append(target)
    lots = instance.usable_lots
    min_set_time_for_machine = float('inf')
    setup_machine, setup_batch = None, None
    family_lock = None
    for i in range(len(lots)):
        lot: Lot = lots[i]
        if family_lock is None or family_lock == lot.actual_step.family:
            family_lock = lot.actual_step.family
            if lot.actual_step.setup_needed == '':
                most_waiting_machine_no_setup = (-1, float('inf'))
                most_waiting_machine = (-1, float('inf'))
                for machine_index, machine in enumerate(lot.waiting_machines):
                    if machine.current_setup == '':
                        if most_waiting_machine_no_setup[1] > machine.free_since:
                            most_waiting_machine_no_setup = (machine_index, machine.free_since)
                        if most_waiting_machine[1] > machine.free_since:
                            most_waiting_machine = (machine_index, machine.free_since)
                if most_waiting_machine_no_setup[0] != -1:
                    return lot.waiting_machines[most_waiting_machine_no_setup[0]], build_batch(lot, lots[i + 1:]), last_sort_time
                else:
                    return lot.waiting_machines[most_waiting_machine[0]], build_batch(lot, lots[i + 1:]), last_sort_time
            else:
                for machine in lot.waiting_machines:
                    if lot.actual_step.setup_needed == machine.current_setup:
                        return machine, build_batch(lot, lots[i + 1:]), last_sort_time
                    else:
                        setup_time_for_machine = Dispatchers.get_setup(lot.actual_step.setup_needed, machine, lot.actual_step.setup_time, instance.setups)
                        if setup_machine is None:
                            setup_machine = machine
                            setup_batch = i
                        if machine.min_runs_left is None:
                            if min_set_time_for_machine > setup_time_for_machine:
                                min_set_time_for_machine = setup_time_for_machine
                                setup_machine = machine
                                setup_batch = i
    return setup_machine, build_batch(lots[setup_batch], lots[setup_batch + 1:]), last_sort_time


def self_supervision(files: Dict[str, List[Dict]], tools_type: list, dataset: str):
    Randomizer().random.seed(0)
    random.seed(0)
    os.environ['PYTHONHASHSEED'] = str(0)
    np.random.seed(0)
    torch.manual_seed(0)
    l4m = False
    plugins = [PierreCostPlugin()]
    run_for = 3600 * 24 * (365 + 30)
    # instance = FileInstance(files, run_for, l4m, plugins)
    if dataset == 'HVLM':
        with open("365days_HVLM.pkl", mode="rb") as file:
            instance = cloudpickle.load(file, fix_imports=True)
            instance.plugins = plugins
            for plugin in plugins:
                plugin.on_sim_init(instance)
    elif dataset == 'LVHM':
        with open("365days_LVHM.pkl", mode="rb") as file:
            instance = cloudpickle.load(file, fix_imports=True)
            instance.plugins = plugins
            for plugin in plugins:
                plugin.on_sim_init(instance)
    lot_rep_self = []
    targets_lot = []
    last_sort_time = -1
    while not instance.done:
        done = instance.next_decision_point()
        instance.print_progress_in_days()
        if done or instance.current_time > run_for:
            break
        machine, lots, last_sort_time = get_lot_representation(instance, instance.current_time, tools_type,
                                                               last_sort_time, lot_rep_self, targets_lot)
        if lots is None:
            instance.usable_lots.clear()
            instance.lot_in_usable.clear()
            instance.next_step()
        else:
            instance.dispatch(machine, lots)
    instance.finalize()
    with torch.no_grad():
        lot_rep_self = torch.cat(lot_rep_self)
        lot_mean = torch.mean(lot_rep_self, dim=0)
        lot_std = torch.std(lot_rep_self, dim=0)
        print(lot_mean)
        print(lot_std)
        normalizer = Normalizer(12)
        normalizer.mean = nn.Parameter(lot_mean)
        normalizer.std = nn.Parameter(lot_std)
        lot_rep_self = normalizer(lot_rep_self)
        torch.save(normalizer.state_dict(), 'normalizer.pt')
    targets_lot = torch.cat(targets_lot)
    perm = torch.randperm(lot_rep_self.size(0))
    idx = perm[:200000]
    dataset = TensorDataset(lot_rep_self[idx], targets_lot[idx])
    dataloader = DataLoader(dataset, batch_size=64)

    master_neural_net = SelfSupervision(12, len(tools_type))
    optim = torch.optim.Adam(master_neural_net.parameters(), lr=1e-4)
    loss_fn = torch.nn.CrossEntropyLoss(label_smoothing=0.1)
    l1_weight = 0.2
    for i in range(20):
        total_loss = 0
        mini_batch_count = 0
        correct = 0
        total = 0
        for X, y in tqdm(dataloader):
            optim.zero_grad()
            y_prime = master_neural_net(X, y)
            loss = loss_fn(y_prime, y)
            l1_penalty = l1_weight * torch.norm(master_neural_net.embedding.embedding, 2)
            loss = loss + l1_penalty
            loss.backward()
            optim.step()
            total_loss += loss.item()
            mini_batch_count += 1
            _, predicted = torch.max(y_prime, 1)
            total += y.size(0)
            correct += (predicted == y).sum().item()
        print(f'loss {total_loss / mini_batch_count} accuracy {correct / total}')
    torch.save(master_neural_net.embedding.state_dict(), 'embedding.pt')


def main():
    ray.init(ignore_reinit_error=True)
    p = argparse.ArgumentParser()
    p.add_argument('--dataset', type=str, default='HVLM')
    p.add_argument('--days', type=int, default=180)
    p.add_argument('--sigma', type=float, default=0.005)
    p.add_argument('--alpha', type=float, default=0.01)
    p.add_argument('--pop_size', type=int, default=64 * 2)#64 * 3)
    p.add_argument('--l2', type=float, default=0.000)
    a = p.parse_args()
    sigma = a.sigma
    alpha = a.alpha
    pop_size = a.pop_size
    l2_penalty = a.l2
    days = a.days
    run = wandb.init(project='new_sim_es_infineon_2',
                     save_code=True,
                     config={
                         'alpha': alpha,
                         'sigma': sigma,
                         'l2': l2_penalty,
                         'pop_size': pop_size
                     })

    sys.stderr.write(f'Loading {a.dataset} for {days} days \n')
    a.days += 365
    sys.stderr.flush()

    files = read_all('../../datasets/SMT2020_' + a.dataset.upper())

    # run_to = lambda i: 3600 * 24 * (30 * (i // 10) + 180)
    run_to = lambda i: 3600 * 24 * days
    Randomizer().random.seed(0)
    random.seed(0)
    os.environ['PYTHONHASHSEED'] = str(0)
    np.random.seed(0)
    torch.manual_seed(0)
    torch.cuda.manual_seed(0)
    torch.cuda.manual_seed_all(0)
    torch.backends.cudnn.deterministic = True
    iter_nb = 40
    best_solution = float('inf')

    plugins = [PierreCostPlugin()]
    master_instance = FileInstance(files, run_to(0), False, plugins)
    tools_type = list(set([x.group if not x.group.startswith('Delay_') else 'Delay' for x in master_instance.machines]))
    tools_type.sort()
    print(tools_type)
    self_supervision(files, tools_type, a.dataset)
    Randomizer().random.seed(0)
    random.seed(0)
    os.environ['PYTHONHASHSEED'] = str(0)
    np.random.seed(0)
    torch.manual_seed(0)
    torch.cuda.manual_seed(0)
    torch.cuda.manual_seed_all(0)
    with torch.no_grad():
        master_neural_net = Network(12, len(tools_type))
        master_neural_net.embedding.load_state_dict(torch.load('embedding.pt'))
        run.save("embedding.pt")
        master_neural_net.normalizer.load_state_dict(torch.load('normalizer.pt'))
        run.save("normalizer.pt")
        if False and os.path.isfile('pretrained.pt'):
            master_neural_net.load_state_dict(torch.load('pretrained.pt'))
        optim = torch.optim.Adam(master_neural_net.inner_net.parameters(), betas=(0.9, 0.98), lr=alpha, eps=1e-4)
        if False and os.path.isfile('optimizer.pt'):
            optim.load_state_dict(torch.load('optimizer.pt'))
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, iter_nb, verbose=True)
        networks = [Network(12, len(tools_type)) for _ in range(pop_size)]
        # needed for LayerNorm
        for k in range(pop_size):
            networks[k].load_state_dict(master_neural_net.state_dict())
            networks[k].train()
        for i in range(iter_nb):
            normal = torch.distributions.normal.Normal(0, sigma)
            optim.zero_grad()
            start = time.time()
            mean = torch.nn.utils.parameters_to_vector(master_neural_net.inner_net.parameters())
            # print(f'size net {mean.shape[0]}')
            noises = normal.sample([pop_size, mean.shape[0]])
            noises[len(noises) // 2:] = -1 * noises[:len(noises) // 2]
            population_params = mean + noises
            for n in range(pop_size):
                torch.nn.utils.vector_to_parameters(population_params[n], networks[n].inner_net.parameters())
            this_iter_seed = random.randint(0, 10)
            output_iter = ray.get(
                [one_pop_iter.remote(this_iter_seed, run_to(i), tools_type, networks[k], a.dataset) for k in
                 range(pop_size)])
            output_iter = np.array(output_iter, dtype=float)
            best_pop = np.argmin(output_iter)
            if output_iter[best_pop] < best_solution:
                best_solution = output_iter[best_pop]
            mean_result = np.mean(output_iter)
            std_result = np.std(output_iter)
            advantage = torch.from_numpy(rank_transformation(output_iter)).unsqueeze(0)
            # advantage = torch.FloatTensor((output_iter - mean_result) / (std_result + 1e-4)).unsqueeze(0)
            print(f"iteration {i} best solution found this iter "
                  f"{output_iter[best_pop]}, with a mean of {mean_result}, median of {np.median(output_iter)}, std of {std_result}, "
                  f"best solution so far {best_solution}, "
                  f"running for {run_to(i) / (3600 * 24)}, "
                  f"it took {time.time() - start} seconds")
            grad = (torch.mm(advantage, noises) / (pop_size * 2 * sigma)).squeeze(0)
            index = 0
            for parameter in master_neural_net.inner_net.parameters():
                num_param = parameter.numel()
                parameter.grad = grad[index:index + num_param].view_as(parameter).data
                parameter.grad.data.clamp_(-1.0, 1.0)
                index += num_param
            optim.step()
            scheduler.step()
            torch.save(master_neural_net.state_dict(), 'pretrained.pt')
            torch.save(optim.state_dict(), 'optimizer.pt')
            end = time.time()
            run.log({
                'iter/iter_time': end - start,
                'general/best_solution': best_solution,
                'iter/mean_iter': mean_result,
                'iter/best_iter_solution': output_iter[best_pop],
                'param/alpha': scheduler.get_last_lr()[0],
                'param/sigma': sigma,
            }, step=i)
            run.save("pretrained.pt")
            run.save("optimizer.pt")
            sigma = sigma * 0.975


if __name__ == '__main__':
    main()
